# CERN to CERN for AI Mechanism Mapping
**Darryl Wright**

## Executive Summary

This paper examines proposals for creating a "CERN for AI" by systematically mapping the mechanisms that make CERN successful onto the artificial intelligence context. Through analysis of six prominent proposals, we find significant differences in their underlying mechanisms and substantial mismatches between CERN's operating assumptions and the AI landscape. Our mechanism mapping reveals that a successful AI analogue would require focusing primarily on AI safety, establishing compute monopolisation through international legislation, and accepting reduced transparency compared to CERN's open science model. The analysis suggests that the MAGIC and TAISC proposals best align with the adapted mechanism requirements, though both face significant implementation challenges.

## Introduction

Recent calls for establishing a "CERN for AI" have emerged as one proposal for governing artificial intelligence systems at an international scale [1-4, 11, 15]. A CERN analogue refers to creating a large international scientific collaboration modelled after the Conseil Européen pour la Recherche Nucléaire (European Organisation for Nuclear Research). However, existing proposals vary considerably in their specifics and remain unclear about how they relate to each other or the extent to which they rely on similar governing mechanisms.

**Mechanism mapping** is a systematic method from evidence-based policy that tests whether successful policies can be adapted to new contexts by examining the contextual assumptions that make them work [6]. This approach helps identify where the original policy's assumptions may not hold in the new environment, potentially undermining its effectiveness.

The proposals also diverge on fundamental questions: Should the organisation promote AI capabilities development or focus specifically on AI safety? While drawing parallels with CERN may effectively communicate the concept of large-scale international AI collaboration, there is a significant risk of developing false confidence that we understand how to implement an international organisation capable of adequately addressing AI's unique challenges.

This paper aims to clarify how the mechanisms that make CERN successful for "nuclear research of a pure scientific and fundamental character" [5] can be adapted for AI governance. We distinguish between **CERN for AI proposals** (focused on advancing AI capabilities) and **CERN for AI Safety proposals** (focused specifically on AI risk mitigation), though we use "CERN for AI" as an umbrella term when the distinction is not critical.

## CERN Context

CERN emerged from the post-war landscape of 1940s Europe when a small group of scientists identified the need for a world-class physics research facility. Their goals were twofold: to stem the talent drain to North America and to foster European unity through scientific collaboration. The devastating nuclear weapons demonstrations of 1945 galvanised the pursuit of fundamental nuclear research that "shall have no concern with work for military requirements" [17].

The organisation's founding convention emphasised transparency, mandating that "results of its experimental and theoretical work shall be published or otherwise made generally available" [17]. CERN operates through contributions from 23 member states, with funding scales determined by Net National Income [18]. The Council governs the organisation, with each member state holding two representatives and one vote. While most decisions require simple majority approval, the Council strives for a  consensus approaching unanimity [19].

CERN has successfully achieved its foundational goals by maintaining world-leading research facilities [20], openly disseminating results [21], and sustaining an expanding international collaboration [22]. **The key to this success is CERN's monopoly on the world's most powerful particle accelerator infrastructure.** The Large Hadron Collider operates at 3.5 TeV energy levels—3.5 times more powerful than the second-most capable accelerator, which was located at Fermilab until its 2011 shutdown [23]. This technological monopoly provides the primary incentive for international collaboration.

Member states derive multiple benefits from participation: research excellence, talent training and attraction, broader application of CERN technologies, industrial returns, and improved diplomatic relations [24]. Countries can also participate through Observer status or Non-Member arrangements, accessing some collaboration benefits without full membership obligations.

## Research Questions and Approach

This study addresses three core questions:

1. How do existing CERN for AI proposals differ in their underlying mechanisms?
2. What contextual assumptions make CERN's mechanism successful, and how do these compare to the AI landscape?
3. What adaptations would be necessary to create an effective CERN for AI mechanism?

Our approach applies evidence-based policy methods to bring this discussion into territory familiar to policymakers. While CERN has succeeded in its particular context, the same institutional design might produce very different outcomes when applied to AI governance. This represents a classic problem of external validity in policy adaptation.

## Methods

### Review of Proposals

We systematically reviewed proposals, drawing parallels between CERN and large international scientific collaborations for AI or AI safety. From the identified proposals, we selected the most prominent for detailed analysis. We developed ten key instruments that such mechanisms might employ and evaluated how each proposal incorporated these elements. These instruments capture the essential design choices that differentiate the various approaches.

**Table 1. Mechanism Instruments and Descriptions**

| Instrument | Description |
|------------|-------------|
| CERN for AI | Proposal focuses on advancing AI capabilities |
| CERN for AI Safety | Proposal focuses specifically on AI safety and risk mitigation |
| Compute Monopoly | Mechanism relies on developing exclusive control over computational resources |
| Model Monopoly | Mechanism relies on developing exclusive control over advanced AI models |
| Data Monopoly | Mechanism relies on developing exclusive control over training datasets |
| Research Monopoly | Mechanism relies on developing exclusive control over frontier AI research |
| Monopoly Legislated | Any monopoly position is established and enforced through international legislation |
| Intergovernmental | Mechanism operates through formal agreements between governments |
| Transparent | Mechanism emphasises transparency and open dissemination of results as default policy |
| Intrinsic Safety Objectives | Mechanism includes AI safety as core organisational mission |

### Mechanism Mapping

We employed a five-step mechanism mapping process [6] to test whether CERN's institutional design could be successfully adapted for AI governance:

1. **Develop a Theory of Change for CERN** - Map how CERN's inputs, activities, and outputs produce its successful outcomes
2. **Identify contextual assumptions** - Determine what conditions must hold for CERN's mechanism to function effectively
3. **Map AI context characteristics** - Analyse the actual features of the AI landscape that differ from CERN's context
4. **Adapt the mechanism** - Modify CERN's approach to address mismatches between assumptions and AI reality
5. **Iterate and refine** - Repeat the analysis to ensure all major design decisions fit the AI context

We used a linear Theory of Change model throughout this process [6, 16]. When adapting the mechanism in step 4, we prioritised options maintaining the greatest fidelity to CERN's original approach while addressing identified mismatches.

## Results

### Review of Proposals

Our analysis included six proposals: Marcus and Slusallek's consortium (circa 2017-2018) [12-14], the AI Safety Project by Ho et al. (2023) [11], the Large-scale Artificial Intelligence Open Network (LAION) [2], the Multilateral AGI Consortium (MAGIC) [4], the Confederation of Laboratories for Artificial Intelligence Research in Europe (CLAIRE) [3], and the Treaty on Artificial Intelligence Safety and Cooperation (TAISC) [15].

The proposals divide equally between those calling for general AI advancement (Marcus & Slusallek, LAION, CLAIRE) and those focused specifically on AI safety (Ho et al., MAGIC, TAISC). Significantly, we found substantial differences in the instruments each proposal employs, with only MAGIC and TAISC relying on identical instrument combinations.

**Table 2. CERN for AI Proposal Comparison**

| Instrument | Marcus & Slusallek | Ho et al. (2023) | LAION | MAGIC | CLAIRE | TAISC |
|------------|-------------------|------------------|--------|--------|---------|--------|
| CERN for AI | x | | x | | x | |
| CERN for AI Safety | | x | | x | | x |
| Compute Monopoly | | | x | x | | x |
| Model Monopoly | | | | x | | x |
| Data Monopoly | | | | | | |
| Research Monopoly | x | x | x | x | x | x |
| Monopoly Legislated | | | | x | | x |
| Intergovernmental | | | | x | | x |
| Transparent | x | | x | | | |
| Intrinsic Safety Objectives | | x | x | x | | x |

### Mechanism Mapping Results

#### Context Mismatches

Our analysis identified significant mismatches between CERN's contextual assumptions and the AI landscape, concentrated primarily in the implementation components (inputs, activities, and outputs) of the institutional design.

**Input-level challenges**: While growing consensus recognises AI as a general-purpose technology with broad potential for societal harm [25-27], and acknowledges that those controlling advanced AI wield considerable economic power [2, 29], critical disagreements persist. Most notably, consensus is lacking on AI's existential or catastrophic risks, which, without robust scientific evidence, appear speculative to many stakeholders [28]. This fundamental disagreement on risk assessment creates uncertainty about organisational purpose, reflected in the diversity of proposals we reviewed.

Additionally, advanced AI development is dominated by private companies operating largely outside direct government control [2], contrasting sharply with CERN's government-funded model.

**Activity-level complications**: AI's general-purpose nature and scaling laws [32,33] make it difficult to determine the risks associated with fundamental research or predict how findings might enable dangerous applications. Consequently, research results may be inappropriate for open dissemination, requiring the organisation to develop secure handling protocols for potentially harmful information.

Safety studies are unlikely to definitively resolve catastrophic risk concerns and would become ongoing organisational activities rather than one-time assessments. If safety research became the primary focus, the organisation would align with our definition of a CERN for AI Safety initiative.

Current AI regulation remains weak or nonexistent [30,31], though several frameworks are under development [34, 35]. The absence of independent international monitoring bodies for harmful AI activities means no external organisation could inspect a CERN for AI equivalent.

**Output-level requirements**: A successful AI collaboration requires world-leading infrastructure that incentivises participation. Current trends point toward supercomputers with specialised AI accelerators as the most effective choice [32, 33, 37-39]. However, maintaining world-leading status would necessitate developing a compute monopoly, and concerns about catastrophic risks correlate directly with computational power [40].

Published safety studies may not adequately address public fears or could contain information about developing harmful AI behaviours that preclude open dissemination.

#### Adapted Mechanism

The mechanism mapping process converged on an **intergovernmental CERN for AI Safety** that relies on developing a compute monopoly to incentivise collaboration. Under scaling law assumptions, this would naturally lead to model and research monopolies as well.

If expert concerns about catastrophic risks [41,42] are taken seriously, protecting the compute monopoly would require governing external compute resources through international legislation. This necessitates an international organisation with authority to establish compliance standards and enforcement power for non-compliant cases.

Transparency remains important, but cannot be the default policy given safety concerns. The mechanism would need to balance scientific scrutiny with security requirements for potentially dangerous research outputs.

This adapted mechanism would produce three key outcomes: hard power that could be exercised by the organisation or member states over non-members, prevention of significant AI-related harms or catastrophic risks, and global proliferation of beneficial AI technologies.

### Comparison to Existing Proposals

The mechanism mapping results align most closely with the MAGIC and TAISC proposals, which best address the identified mismatches while maintaining fidelity to CERN's original institutional design. Among these, TAISC provides considerably more concrete implementation details, including a draft international treaty and specific compute limits for both the CERN-like organisation and external entities.

## Discussion

### Critical Design Considerations

**Consensus and political sustainability**: Unlike fundamental nuclear research, where economic benefits are less tangible and immediate, AI development offers clear competitive advantages that make unilateral action attractive. Therefore, any compute monopoly must make independent competition economically prohibitive to adequately discourage defection from the collaborative framework.

**Transparency versus security trade-offs**: CERN's positive public image benefits significantly from its transparency policies. However, AI's broad potential for harmful applications and difficulties in predicting future system capabilities mean that safety studies may not adequately address public concerns. Studies that might otherwise alleviate these fears could require restricted publication due to dual-use concerns, potentially undermining public support and political sustainability.

**Power dynamics and collaboration**: Control over advanced AI capabilities could grant the organisation substantial hard power, fundamentally altering both public perception and collaboration dynamics compared to CERN's more benevolent scientific mission.

### Implementation Challenges

The adapted mechanism faces several implementation hurdles. Establishing meaningful international regulation requires enforcement mechanisms for compute limits outside the collaboration—a coercive element absent from CERN's voluntary cooperation model. Robust compute governance and inspection protocols would be essential to prevent circumvention.

Concentrating frontier model training within a single organisation could avoid harmful race dynamics, but this approach represents a significant departure from CERN's competitive advantage through superior infrastructure rather than legal monopolisation.

### Study Limitations

This analysis has several important limitations. The linear Theory of Change representation oversimplifies complex feedback relationships and doesn't clearly indicate which assumptions affect specific mechanism components. The depth of our analysis was constrained by available time and resources, potentially missing important nuances in both CERN's operation and the AI context.

The validity of our conclusions depends heavily on how accurately we captured both CERN's actual success mechanisms and the AI governance landscape. Given the rapidly evolving nature of AI technology and policy, some contextual assumptions may become outdated quickly.

Despite these limitations, the systematic approach offers several strengths: it explicitly details assumptions underlying each proposal, surfaces high-leverage intervention points for pursuing international AI governance, and provides a framework for evaluating future proposals against realistic implementation requirements.

## Conclusions

Existing proposals for large international AI collaborations that draw parallels with CERN differ substantially from each other and from CERN's actual operating mechanisms. The parallel with CERN extends only loosely to the concept of large international research collaboration—the underlying institutional designs diverge significantly from CERN's successful model.

Our analysis suggests that an effective CERN for AI should focus primarily on AI safety rather than general capabilities advancement. The AI context demands significant adaptations to CERN's mechanism, particularly regarding transparency policies, monopoly establishment, and enforcement mechanisms. The mechanism derived from mapping CERN onto the AI context aligns most closely with the MAGIC and TAISC proposals.

However, implementing such an organisation would require overcoming substantial political, technical, and economic challenges that exceed those faced by CERN's founders. Success would depend on achieving international consensus on AI risks, developing effective compute governance protocols, and maintaining public legitimacy despite reduced transparency.

Policymakers considering international AI governance approaches should recognise that successful adaptation of CERN's model requires fundamental modifications rather than superficial institutional mimicry. The path forward likely involves building consensus on AI safety priorities, developing robust scientific evidence regarding catastrophic risks, and creating protocols that demonstrate safety while preventing harmful applications from entering the public domain.

## Acknowledgements

This work was made possible through a Long-Term Future Fund (EA Funds) grant. DW also wishes to acknowledge the support this work received through participation in the BlueDot Impact AI Governance course. We are grateful to David Atkinson, Will Gantt, Marta Bienkiewicz, Joe Rogero, Javier Prieto, Moritz von Knebel, Nicole Mutung'a, Joan O'Bryan, Jason Danker, Jacob Haimes, Kay Kozaronek, Mario Giulio Bertorelli, Alex Gray, Coby Joseph, Tomáš Dulka, Christopher Phenicie, Peter Barnett for helpful feedback and discussions. Development of the CERN ToC was greatly aided by EA forum user SWK's post on AI-Relevant Regulation: CERN.

## References

[1] https://www.ft.com/content/7c30ea28-2895-44c2-9a2d-c31ea7fa27e7

[2] https://www.openpetition.eu/petition/online/securing-our-digital-future-a-cern-for-open-source-large-scale-ai-research-and-its-safety

[3] https://claire-ai.org/vision/

[4] https://time.com/6314045/prevent-ai-disaster-nuclear-catastrophe/

[5] http://cds.cern.ch/record/1475694/files/1955_e_cm-p00043028.pdf

[6] Williams, M. J. (2017). External Validity and Policy Adaptation: A Five-step Guide to Mechanism Mapping. Policy Memo, Blavatnik School of Government, Oxford University, Oxford, UK. https://www.bsg.ox.ac.uk/research/publications/external-validity-and-policy-adaptation-0

[7] https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Triad-Report.pdf

[8] https://epochai.org/blog/scaling-laws-literature-review

[9] https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/

[10] https://lsag.web.cern.ch/LSAG-Report.pdf

[11] Ho, L., Barnhart, J., Trager, R., Bengio, Y., Brundage, M., Carnegie, A., ... & Snidal, D. (2023). International Institutions for Advanced AI. arXiv preprint arXiv:2307.04699.

[12] https://www.reframetech.de/en/2018/05/22/opinion-piece-why-we-need-a-cern-for-ai/

[13] https://www.oecd-forum.org/posts/28452-artificial-intelligence-and-digital-reality-do-we-need-a-cern-for-ai

[14] https://www.nytimes.com/2017/07/29/opinion/sunday/artificial-intelligence-is-stuck-heres-how-to-move-it-forward.html

[15] https://taisc.org/overview

[16] Davies, R. (2018). Representing theories of change: Technical challenges with evaluation consequences. Journal of Development Effectiveness, 10(4), 438-461.

[17] https://council.web.cern.ch/en/content/convention-establishment-european-organization-nuclear-research

[18] https://cds.cern.ch/record/2864553/files/English.pdf

[19] https://home.cern/about/who-we-are/our-governance

[20] https://home.cern/science/accelerators

[21] https://cds.cern.ch/collection/CERN%20Published%20Articles?ln=en

[22] Voss, R. (2015, May). CERN: A European laboratory for a global project. In Journal of Physics: Conference Series (Vol. 623, No. 1, p. 012028). IOP Publishing. https://iopscience.iop.org/article/10.1088/1742-6596/623/1/012028/pdf

[23] https://www.fnal.gov/pub/tevatron/tevatron-accelerator.html

[24] https://www.ukri.org/wp-content/uploads/2021/12/091221-STFC-EvaluationOfBenefitsUKDerivesFromCERN-MainReport.pdf#page=5

[25] https://aisafetyfundamentals.com/blog/overview-of-ai-risk-exacerbation

[26] Hendrycks, D., Carlini, N., Schulman, J., & Steinhardt, J. (2021). Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916.

[27] https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios

[28] https://www.chathamhouse.org/2023/06/nuclear-governance-model-wont-work-ai

[29] https://openai.com/charter

[30] Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O'Keefe, C., Whittlestone, J., ... & Wolf, K. (2023). Frontier AI Regulation: Managing Emerging Risks to Public Safety. arXiv preprint arXiv:2307.03718. https://arxiv.org/pdf/2307.03718.pdf

[31] Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., ... & Dafoe, A. (2023). Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324. https://arxiv.org/pdf/2305.15324.pdf

[32] https://epochai.org/blog/scaling-laws-literature-review

[33] https://arxiv.org/pdf/2307.03201.pdf

[34] https://artificialintelligenceact.eu/the-act/

[35] https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/

[36] O'Keefe, C., Cihon, P., Garfinkel, B., Flynn, C., Leung, J., & Dafoe, A. (2020, February). The windfall clause: Distributing the benefits of AI for the common good. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (pp. 327-331).

[37] https://cset.georgetown.edu/wp-content/uploads/AI-and-Compute-How-Much-Longer-Can-Computing-Power-Drive-Artificial-Intelligence-Progress_v2.pdf

[38] https://epochai.org/blog/trends-in-machine-learning-hardware

[39] https://cset.georgetown.edu/publication/the-ai-triad-and-what-it-means-for-national-security-strategy/

[40] Bio-anchors. https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP

[41] https://www.youtube.com/watch?v=yl2nlejBcg0&t=105s&ab_channel=StanfordHAI

[42] https://aiimpacts.org/what-do-ml-researchers-think-about-ai-in-2022/

[43] https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/

[44] Shavit, Y. (2023). What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring. arXiv preprint arXiv:2303.11341. https://arxiv.org/pdf/2303.11341.pdf
